---
title: "Data Science Capstone"
author: "John Wright"
date: "Monday, March 09, 2015"
output: html_document
---

# Prepare environment
```{r}
# Libraries
library(tm)
library(SnowballC)
library(openNLP)
library(NLP)
library(RWeka)
library(slam)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
library(reshape2)

# additional resource
# http://nlpwp.org/book/index.xhtml
# http://www.nltk.org/book/?=
# http://www.e-booksdirectory.com/listing.php?category=281
# http://simplystatistics.org/2015/02/05/johns-hopkins-data-science-specialization-top-performers/

# Set wd to the right computer
if (Sys.getenv("USERNAME") == "jwright") {
      setwd("C:/Users/jwright/Desktop/p6/Data-Science-Capstone")
} else setwd("D:/Coursera/Data-Science-Capstone")

# Load data
set.seed(123)

# Function to shrink the data to 1/n of its original size
shrunk <- function(x) {
      if (class(x) == "data.frame") {
            nrow(x) / 20
      } else length(x) / 20
}

# Load and sample each file
en.t <- readLines("./en_US/en_US.twitter.txt")
en.t <- sample(en.t, size = shrunk(en.t))

en.b <- readLines("./en_US/en_US.blogs.txt")
en.b <- sample(en.b, size = shrunk(en.b))

en.n <- file("./en_US/en_US.news.txt", open = "rb")
en.n <- readLines(en.n, encoding = "UTF-8")
en.n <- sample(en.n, size = shrunk(en.n))

```

Starting help:
http://www.exegetic.biz/blog/2013/09/text-mining-the-complete-works-of-william-shakespeare/
http://www.unt.edu/rss/class/Jon/Benchmarks/TextMining_L_JDS_Jan2014.pdf

Other helpful data sources ideas:
*     Table of names (all languages)
*     Table of locations (perhaps associate them with a language)
*     Table of emoticons
*     Table of common slang in English, German, Russian and Finnish
*     Table of common abbreviations
*     Table of profanity
*     Sentiment Analysis
*     Table of topics with each word mapped to a type of topic (or probabilities of words being associated with each topic)

# Cleaning
```{r, echo=FALSE}

# Convert each data set to a corpus and clean the data
corpusCleaner <- function(x) {
      x <- Corpus(VectorSource(x))
      x <- tm_map(x, content_transformer(tolower))
      x <- tm_map(x, removePunctuation)
      x <- tm_map(x, removeNumbers)
      x <- tm_map(x, content_transformer(function(z) gsub("[^0-9A-Za-z///' ]", "", z)))
      x <- tm_map(x, stemDocument)
      x <- tm_map(x, stripWhitespace)
      x
}

# Make a corpus with each data set.
en.b <- corpusCleaner(en.b)
en.n <- corpusCleaner(en.n)
en.t <- corpusCleaner(en.t)

# Combine the corpora into 1.
en.all <- do.call(function(...) c(..., recursive = T), list(en.b, en.n, en.t))

# Create a term document matrix
tdm <- TermDocumentMatrix(en.all)

# Create a term document matrix that includes only the common tokens
tdm.c <- removeSparseTerms(tdm, 0.99) # add values to play with size. had this at .99 before

# Create a dense term document matrix from the common tokens
tdm.d <- as.matrix(tdm.c)

# Create a frequency table of the dense matrix
freq <- data.frame(word = rownames(tdm.d), count = rowSums(tdm.d), row.names = NULL)
freq <- freq[order(-freq$count), ]

# Create N-grams
# http://stackoverflow.com/questions/19615181/finding-ngrams-in-r-and-comparing-ngrams-across-corpora
bgram <- function(x) {NGramTokenizer(x, Weka_control(min = 2, max = 2))}
tgram <- function(x) {NGramTokenizer(x, Weka_control(min = 3, max = 3))}

tdm.2 <- TermDocumentMatrix(en.all, control = list(tokenize = bgram))
# sparsity value may need tweaking, once comfortable have it overwrite tdm.2
tdm.2t <- removeSparseTerms(tdm.2, 0.999)

tdm.3 <- TermDocumentMatrix(en.all, control = list(tokenize = tgram))
# sparsity value may need tweaking, once comfortable have it overwrite tdm.3
tdm.3t <- removeSparseTerms(tdm.3, 0.99995)

# Frequency tables of bigrams and trigrams
bgram.freq <- as.matrix(rollup(tdm.2t, 2, na.rm = T, FUN = sum))
bgram.freq <- data.frame(word = rownames(bgram.freq), count = bgram.freq[, 1], row.names = NULL)
bgram.freq <- bgram.freq[order(-bgram.freq$count), ]

tgram.freq <- as.matrix(rollup(tdm.3t, 2, na.rm = T, FUN = sum))
tgram.freq <- data.frame(word = rownames(tgram.freq), count = tgram.freq[, 1], row.names = NULL)
tgram.freq <- tgram.freq[order(-tgram.freq$count), ]

```

# Exploratory Analysis
```{r}
# http://beyondvalence.blogspot.com/2014/01/text-mining-4-performing-term.html
# http://rstudio-pubs-static.s3.amazonaws.com/39474_1e1930a37b6f4b1bbfaf916db55b0397.html
findFreqTerms(tdm, 99)

# Create frequency tables of unigrams, bigrams, and trigrams
# Unigrams with frequencies in the 50th percentile
termFreq <- freq[freq$count >= quantile(freq$count, 0.5), ] # above the 50th percentile
qplot(x = word, y = count, data = termFreq, 
      main = "Term Frequencies", geom = "bar", xlab = "Unigrams", ylab = "Frequency", 
      stat = "identity", fill = I("royalblue4")) + coord_flip()

# Bigrams with frequencies in the 95th percentile
termFreq <- with(bgram.freq, bgram.freq[count >= quantile(count, 0.95), ])
qplot(x = word, y = count, data = termFreq, 
      main = "Bigram Frequencies", geom = "bar", xlab = "Bigrams", ylab = "Frequency",
      stat = "identity", fill = I("red4")) + coord_flip()

# Trigrams with frequencies in the 99.5th percentile
termFreq <- with(tgram.freq, tgram.freq[count >= quantile(count, 0.995) ,])
qplot(x = word, y = count, data = termFreq, main = "Trigram Frequencies",
      geom = "bar", xlab = "Trigrams", ylab = "Frequency",
      stat = "identity", fill = I("seagreen4")) + coord_flip()

# How many unique words do you need in a frequency sorted dictionary 
# to cover 50% of all word instances in the language? 90%?
# http://wordnet.princeton.edu/

totalWords <- removeSparseTerms(tdm, 0.9999)

uWords <- rownames(totalWords)
length(uWords) # Number of unique words

totalWords <- rowSums(as.matrix(inspect(totalWords[1:2000, ])))

```




