---
title: "Data Science Capstone"
author: "John Wright"
date: "Monday, March 09, 2015"
output: html_document
---

# Prepare environment
```{r}
# Libraries
library(tm)
library(SnowballC)
library(openNLP)
library(NLP)
library(RWeka)
library(slam)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
library(reshape2)

# Set wd to the right computer
if (Sys.getenv("USERNAME") == "jwright") {
      setwd("C:/Users/jwright/Desktop/p6/Data-Science-Capstone")
} else setwd("D:/Coursera/Data-Science-Capstone")

# Load data
set.seed(123)

shrunk <- function(x) {
      if (class(x) == "data.frame") {
            nrow(x) / 10
      } else length(x) / 10
}

en.t <- readLines("./en_US/en_US.twitter.txt")
en.t <- sample(en.t, size = shrunk(en.t))

en.b <- readLines("./en_US/en_US.blogs.txt")
en.b <- sample(en.b, size = shrunk(en.b))

en.n <- readLines("./en_US/en_US.news.txt")
en.n <- sample(en.n, size = shrunk(en.n))

```

Starting help:
http://www.exegetic.biz/blog/2013/09/text-mining-the-complete-works-of-william-shakespeare/
http://www.unt.edu/rss/class/Jon/Benchmarks/TextMining_L_JDS_Jan2014.pdf

Other helpful data sources ideas:
*     Table of names (all languages)
*     Table of locations (perhaps associate them with a language)
*     Table of emoticons
*     Table of common slang in English, German, Russian and Finnish
*     Table of common abbreviations
*     Table of profanity
*     Sentiment Analysis
*     Table of topics with each word mapped to a type of topic (or probabilities of words being associated with each topic)

# Cleaning
```{r, echo=FALSE}

# Convert each data set to a corpus and clean the data
corpusCleaner <- function(x) {
      x <- Corpus(VectorSource(x))
      x <- tm_map(x, content_transformer(tolower))
      x <- tm_map(x, removePunctuation)
      x <- tm_map(x, removeNumbers)
      x <- tm_map(x, removeWords, stopwords("english"))
      x <- tm_map(x, content_transformer(function(z) gsub("[^[:alnum:]///' ]", "", z)))
      x <- tm_map(x, stemDocument)
      x <- tm_map(x, stripWhitespace)
      x
}

# Make a corpus with each data set.
en.b <- corpusCleaner(en.b)
en.n <- corpusCleaner(en.n)
en.t <- corpusCleaner(en.t)

# Combine the corpora into 1.
en.all <- do.call(function(...) c(..., recursive = T), list(en.b, en.n, en.t))

# Create a term document matrix
tdm <- TermDocumentMatrix(en.all)

# Create a term document matrix that includes only the common tokens
tdm.c <- removeSparseTerms(tdm, 0.99) # add values to play with size

# Create a dense term document matrix from the common tokens
tdm.d <- as.matrix(tdm.c)

# Create a wordcloud because why not
# pal <- brewer.pal(9, "BuGn")[-(1:4)]
# wordcloud(rownames(tdm.d), rowSums(tdm.d), min.freq = 1, color = pal)

# Create a frequency table of the dense matrix
freq <- data.frame(word = rownames(tdm.d), count = rowSums(tdm.d), row.names = NULL)
freq <- freq[order(-freq$count), ]

# Create N-grams
# http://stackoverflow.com/questions/19615181/finding-ngrams-in-r-and-comparing-ngrams-across-corpora
bgram <- function(x) {NGramTokenizer(x, Weka_control(min = 2, max = 2))}
tgram <- function(x) {NGramTokenizer(x, Weka_control(min = 3, max = 3))}

tdm.2 <- TermDocumentMatrix(en.all, control = list(tokenize = bgram))
# tdm.2 <- removeSparseTerms(tdm.2, 0.99) # this param needs tweaking

tdm.3 <- TermDocumentMatrix(en.all, control = list(tokenize = tgram))
# tdm.3 <- removeSparseTerms(tdm.3, 0.99) # this param needs tweaking


```


