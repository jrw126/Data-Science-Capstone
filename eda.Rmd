---
title: "Data Science Capstone"
author: "John Wright"
date: "Monday, March 09, 2015"
output: html_document
---

# Prepare environment
```{r}
# Libraries
if(Sys.getenv("JAVA_HOME")!=""){
    Sys.setenv(JAVA_HOME="")
}

library(tm)
library(SnowballC)
library(openNLP)
library(NLP)
library(RWeka)
library(slam)
library(wordcloud)
library(RColorBrewer)
library(ggplot2); library(scales)
library(reshape2)

# additional resource
# http://nlpwp.org/book/index.xhtml
# http://www.nltk.org/book/?=
# http://www.e-booksdirectory.com/listing.php?category=281
# http://simplystatistics.org/2015/02/05/johns-hopkins-data-science-specialization-top-performers/

# Set wd to the right computer
if (Sys.getenv("USERNAME") == "jwright") {
      setwd("C:/Users/jwright/Desktop/p6/Data-Science-Capstone")
} else setwd("D:/Coursera/Data-Science-Capstone")

# Load data
set.seed(123)

# Function to shrink the data to 1/n of its original size
shrunk <- function(x) {
      if (class(x) == "data.frame") {
            nrow(x) / 20
      } else length(x) / 20
}

# Load and sample each file
en.t <- readLines("./en_US/en_US.twitter.txt")
en.t <- sample(en.t, size = shrunk(en.t))

en.b <- readLines("./en_US/en_US.blogs.txt")
en.b <- sample(en.b, size = shrunk(en.b))

en.n <- file("./en_US/en_US.news.txt", open = "rb")
en.n <- readLines(en.n, encoding = "UTF-8")
en.n <- sample(en.n, size = shrunk(en.n))

```

Starting help:
http://www.exegetic.biz/blog/2013/09/text-mining-the-complete-works-of-william-shakespeare/
http://www.unt.edu/rss/class/Jon/Benchmarks/TextMining_L_JDS_Jan2014.pdf

Other helpful data sources ideas:
*     Table of names (all languages)
*     Table of locations (perhaps associate them with a language)
*     Table of emoticons
*     Table of common slang in English, German, Russian and Finnish
*     Table of common abbreviations
*     Table of profanity
*     Sentiment Analysis
*     Table of topics with each word mapped to a type of topic (or probabilities of words being associated with each topic)

# Cleaning
```{r, echo=FALSE}

# Convert each data set to a corpus and clean the data
corpusCleaner <- function(x) {
      x <- Corpus(VectorSource(x))
      x <- tm_map(x, content_transformer(tolower))
      x <- tm_map(x, removePunctuation)
      x <- tm_map(x, removeNumbers)
      x <- tm_map(x, content_transformer(function(z) gsub("[^0-9A-Za-z///' ]", "", z)))
      x <- tm_map(x, stemDocument)
      x <- tm_map(x, stripWhitespace)
      x
}

# Make a corpus with each data set.
en.b <- corpusCleaner(en.b)
en.n <- corpusCleaner(en.n)
en.t <- corpusCleaner(en.t)

# Combine the corpora into 1.
en.all <- do.call(function(...) c(..., recursive = T), list(en.b, en.n, en.t))

# Create a term document matrix
tdm <- TermDocumentMatrix(en.all)

# Create a term document matrix that includes only the common tokens
tdm.c <- removeSparseTerms(tdm, 0.99) # add values to play with size. had this at .99 before

# Create a dense term document matrix from the common tokens
tdm.d <- as.matrix(tdm.c)

# Create a frequency table of the dense matrix
freq <- data.frame(word = rownames(tdm.d), count = rowSums(tdm.d), row.names = NULL)
freq <- freq[order(-freq$count), ]

# Create N-grams
# http://stackoverflow.com/questions/19615181/finding-ngrams-in-r-and-comparing-ngrams-across-corpora
bgram <- function(x) {NGramTokenizer(x, Weka_control(min = 2, max = 2))}
tgram <- function(x) {NGramTokenizer(x, Weka_control(min = 3, max = 3))}

tdm.2 <- TermDocumentMatrix(en.all, control = list(tokenize = bgram))
# sparsity value may need tweaking, once comfortable have it overwrite tdm.2
tdm.2t <- removeSparseTerms(tdm.2, 0.999)

tdm.3 <- TermDocumentMatrix(en.all, control = list(tokenize = tgram))
# sparsity value may need tweaking, once comfortable have it overwrite tdm.3
tdm.3t <- removeSparseTerms(tdm.3, 0.99995)

# Frequency tables of bigrams and trigrams
bgram.freq <- as.matrix(rollup(tdm.2t, 2, na.rm = T, FUN = sum))
bgram.freq <- data.frame(word = rownames(bgram.freq), count = bgram.freq[, 1], row.names = NULL)
bgram.freq <- bgram.freq[order(-bgram.freq$count), ]

tgram.freq <- as.matrix(rollup(tdm.3t, 2, na.rm = T, FUN = sum))
tgram.freq <- data.frame(word = rownames(tgram.freq), count = tgram.freq[, 1], row.names = NULL)
tgram.freq <- tgram.freq[order(-tgram.freq$count), ]

```

# Exploratory Analysis
Some words are more frequent than others - what are the distributions of word frequencies? 
What are the frequencies of 2-grams and 3-grams in the dataset? 
```{r}
# http://beyondvalence.blogspot.com/2014/01/text-mining-4-performing-term.html
# http://rstudio-pubs-static.s3.amazonaws.com/39474_1e1930a37b6f4b1bbfaf916db55b0397.html
findFreqTerms(tdm, 99)

# Create frequency tables of unigrams, bigrams, and trigrams
# Unigrams with frequencies in the 50th percentile
termFreq <- freq[freq$count >= quantile(freq$count, 0.5), ] # above the 50th percentile
qplot(x = word, y = count, data = termFreq, 
      main = "Term Frequencies", geom = "bar", xlab = "Unigrams", ylab = "Frequency", 
      stat = "identity", fill = I("royalblue4")) + coord_flip()

# Bigrams with frequencies in the 95th percentile
termFreq <- with(bgram.freq, bgram.freq[count >= quantile(count, 0.95), ])
qplot(x = word, y = count, data = termFreq, 
      main = "Bigram Frequencies", geom = "bar", xlab = "Bigrams", ylab = "Frequency",
      stat = "identity", fill = I("red4")) + coord_flip()

# Trigrams with frequencies in the 99.5th percentile
termFreq <- with(tgram.freq, tgram.freq[count >= quantile(count, 0.995) ,])
qplot(x = word, y = count, data = termFreq, main = "Trigram Frequencies",
      geom = "bar", xlab = "Trigrams", ylab = "Frequency",
      stat = "identity", fill = I("seagreen4")) + coord_flip()
```

```{r}
# Too large to work with as a matrix, so I am doing this iteratively and writing the results to a file
# This takes a long time to run.
# totalWords <- removeSparseTerms(tdm, 0.9999)
# uWords <- rownames(totalWords)
# data <- c()
# for (word in 1:length(uWords)) { 
#       r <- c(uWords[word], sum(totalWords[uWords[word], ]))
#       data <- c(data, r)
# }
# data <- data.frame(word = data[seq(1, length(data), 2)], count = as.integer(data[seq(2, length(data), 2)]))
# 
# write.table(data, "completeUnigramFreqTable.txt", sep = "\t", row.names = F)

# This code creates the file used in the chunk below
```

How many unique words do you need in a frequency sorted dictionary 
to cover 50% of all word instances in the language? 90%?
```{r}
ugram.freq <- read.delim("completeUnigramFreqTable.txt", sep = "\t", header = T, stringsAsFactors = F)
ugram.freq <- ugram.freq[order(-ugram.freq$count), ]
ugram.freq$cumsum <- cumsum(ugram.freq$count)
ugram.freq$pctcoverage <- ugram.freq$cumsum / sum(ugram.freq$count)

pcts <- seq(.1, .9, .1)
pts <- sapply(pcts, function(x) with(ugram.freq, which.max(pctcoverage[pctcoverage <= x])))

qplot(x = pctcoverage, y = 1:nrow(ugram.freq), data = ugram.freq,
      main = "Percent of Language Coverage", xlab = "Percentage", ylab = "Number of Unique Words") +
      ggtitle(expression(atop(bold("Percent of Word Instance Coverage"), atop(italic("per unique word"), "")))) + 
      geom_point(colour = I("cadetblue3")) +
      geom_point(data = ugram.freq[pts, ], aes(x = pctcoverage, y = pts), colour = I("darkred"), size = 4) + 
      geom_text(data = ugram.freq[pts, ], aes(x = pctcoverage, y = pts + 170, label = comma(pts)), colour = "darkred") +
      scale_x_continuous(breaks = seq(0, 1, .1), labels = percent) + 
      scale_y_continuous(breaks = seq(0, 9000, 1000), labels = comma)

```

How do you evaluate how many of the words come from foreign languages?
In theory, words from foreign languages along with things like misspellings and other oddities would be distributed very sparsly amongst the data. Therefore, it should be filtered out by the removeSparseTerms function.

Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

To answer this, it would help to have a list of all words in the English language.
http://wordnet.princeton.edu/
```{r}
# Function to read the Princeton WordNet files
wordnetReader <- function(x, n) {
      d <- readLines(x)
      d <- d[30:length(d)] # First 29 lines consist of licensing info
      d <- strsplit(d, split = " ") # Space delimited
      d <- rapply(d, function(z) z[n]) # Extract the word from each row only
      d
}

# All relevant WordNet files
wordnets <- paste0("./dict/", 
                   c("data.noun", "index.noun",
                     "data.adj", "index.adj",
                     "data.verb", "index.verb",
                     "data.adv", "index.adv"))

# Scrape data from all WordNet files and remove duplicates
wordnet <- c()
for (w in wordnets) {
      if (grepl("data", w)) {
             n <- 5
      } else n <- 1
      i <- wordnetReader(w, n)
      wordnet <- c(wordnet, i)
}
wordnet <- unique(wordnet) # Remove duplicates

# Identify bigrams and trigrams in the WordNet list



```

# Modeling
http://www.cs.columbia.edu/~mcollins/
```{r}
# Create a probability distribution for the unigram, bigram and trigram set.
ugram.freq$p <- ugram.freq$count / sum(ugram.freq$count)

```



